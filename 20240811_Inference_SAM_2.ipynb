{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59ef6d9c-0c31-4afb-9639-cbb1483dd981",
   "metadata": {},
   "source": [
    "https://medium.com/@sagieppel/train-fine-tune-segment-anything-2-sam-2-in-60-lines-of-code-928dd29a63b3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b96d471-2e9c-480d-8533-cbb0dde0ab3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OLD_GPU False, USE_FLASH_ATTN True, MATH_KERNEL_ON False\n"
     ]
    }
   ],
   "source": [
    "# segment image region using  fine tune model\n",
    "# See Train.py on how to fine tune/train the model\n",
    "import numpy as np\n",
    "import torch\n",
    "import cv2\n",
    "import os\n",
    "from sam2.build_sam import build_sam2\n",
    "from sam2.sam2_image_predictor import SAM2ImagePredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3cb8677-85ef-44a0-bb06-58c9832a6d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use bfloat16 for the entire script (memory efficient)\n",
    "torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16).__enter__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0248319-428b-4bea-b1ac-7f4325b92a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = r\"sample_image.jpg\" # path to image\n",
    "mask_path = r\"sample_mask.png\" # path to mask, the mask will define the image region to segment\n",
    "num_samples = 30 # number of points/segment to sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b193c2d-5957-4fba-9f7d-a46fd3d5e9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image(image_path, mask_path): # read and resize image and mask\n",
    "        img = cv2.imread(image_path)[...,::-1]  # read image\n",
    "        mask = cv2.imread(mask_path,0)\n",
    "        r = np.min([1024 / img.shape[1], 1024 / img.shape[0]])\n",
    "        img = cv2.resize(img, (int(img.shape[1] * r), int(img.shape[0] * r)))\n",
    "        mask = cv2.resize(mask, (int(mask.shape[1] * r), int(mask.shape[0] * r)),interpolation=cv2.INTER_NEAREST)\n",
    "        return img, mask\n",
    "def get_points(mask,num_points): # Sample points inside the input mask\n",
    "        points=[]\n",
    "        for i in range(num_points):\n",
    "            coords = np.argwhere(mask > 0)\n",
    "            yx = np.array(coords[np.random.randint(len(coords))])\n",
    "            points.append([[yx[1], yx[0]]])\n",
    "        return np.array(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72c25486-9eb3-4cf7-8595-2269e344c74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read image and sample points\n",
    "image,mask = read_image(image_path, mask_path)\n",
    "input_points = get_points(mask,num_samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7abf3c-cd72-475e-ae4c-9a63d29d4a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model you need to have pretrained model already made\n",
    "sam2_checkpoint = \"sam2_hiera_small.pt\" # \"sam2_hiera_large.pt\"\n",
    "model_cfg = \"sam2_hiera_s.yaml\" # \"sam2_hiera_l.yaml\"\n",
    "sam2_model = build_sam2(model_cfg, sam2_checkpoint, device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95a04b71-993d-473a-9842-e5ce7ae0efd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model you need to have pretrained model already made\n",
    "sam2_checkpoint = \"sam2_hiera_large.pt\" # path to model weight\n",
    "model_cfg = \"sam2_hiera_l.yaml\" # model config\n",
    "sam2_model = build_sam2(model_cfg, sam2_checkpoint, device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e6b2c8a-8fa8-4244-b56b-ad736233cc3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build net and load weights\n",
    "predictor = SAM2ImagePredictor(sam2_model)\n",
    "predictor.model.load_state_dict(torch.load(\"l_model_6047.torch\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "925e60d5-af14-4790-bb0b-97f88a8bb21d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\workspace\\20240810_Segment_Anything_2\\segment-anything-2\\sam2\\modeling\\backbones\\hieradet.py:68: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  x = F.scaled_dot_product_attention(\n"
     ]
    }
   ],
   "source": [
    "# predict mask\n",
    "\n",
    "with torch.no_grad():\n",
    "    predictor.set_image(image)\n",
    "    masks, scores, logits = predictor.predict(\n",
    "        point_coords=input_points,\n",
    "        point_labels=np.ones([input_points.shape[0],1])\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0835a5aa-a511-47ba-9158-e6b319401bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Short predicted masks from high to low score\n",
    "\n",
    "np_masks = np.array(masks[:,0])\n",
    "np_scores = scores[:,0]\n",
    "shorted_masks = np_masks[np.argsort(np_scores)][::-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee2bdd4e-a5b1-4b05-81da-3a6346ed6a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_masks = np.array(masks[:,0].numpy()) if isinstance(masks, torch.Tensor) else np.array(masks[:,0])\n",
    "np_scores = scores[:,0].float().numpy() if isinstance(scores, torch.Tensor) else np.array(scores[:,0])\n",
    "shorted_masks = np_masks[np.argsort(np_scores)][::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d79ca4c0-e184-4259-bdb4-daaa613e3af2",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "arrays used as indices must be of integer (or boolean) type",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (mask\u001b[38;5;241m*\u001b[39moccupancy_mask)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m/\u001b[39mmask\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m0.15\u001b[39m: \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m      8\u001b[0m mask[occupancy_mask]\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[1;32m----> 9\u001b[0m \u001b[43mseg_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m=\u001b[39mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     10\u001b[0m occupancy_mask[mask]\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mIndexError\u001b[0m: arrays used as indices must be of integer (or boolean) type"
     ]
    }
   ],
   "source": [
    "# Stitch predicted mask into one segmentation mask\n",
    "\n",
    "seg_map = np.zeros_like(shorted_masks[0],dtype=np.uint8)\n",
    "occupancy_mask = np.zeros_like(shorted_masks[0],dtype=bool)\n",
    "for i in range(shorted_masks.shape[0]):\n",
    "    mask = shorted_masks[i]\n",
    "    if (mask*occupancy_mask).sum()/mask.sum()>0.15: continue\n",
    "    mask[occupancy_mask]=0\n",
    "    seg_map[mask]=i+1\n",
    "    occupancy_mask[mask]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cb1b874e-50f6-48f2-bfcd-c7009861ee67",
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_map = np.zeros_like(shorted_masks[0], dtype=np.uint8)\n",
    "occupancy_mask = np.zeros_like(shorted_masks[0], dtype=bool)\n",
    "\n",
    "for i in range(shorted_masks.shape[0]):\n",
    "    mask = shorted_masks[i]\n",
    "    if (mask * occupancy_mask).sum() / mask.sum() > 0.15:\n",
    "        continue\n",
    "    \n",
    "    # Convert mask to boolean when needed\n",
    "    mask_bool = mask.astype(bool)\n",
    "    \n",
    "    mask_bool[occupancy_mask] = False  # Set overlapping areas to False in the mask\n",
    "    seg_map[mask_bool] = i + 1         # Use boolean mask to index seg_map\n",
    "    occupancy_mask[mask_bool] = True   # Update occupancy_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5b8c84d1-44ff-4bd8-a91e-f580ec263022",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create colored annotation map\n",
    "height, width = seg_map.shape\n",
    "\n",
    "# Create an empty RGB image for the colored annotation\n",
    "rgb_image = np.zeros((height, width, 3), dtype=np.uint8)\n",
    "# Map each class number to a random  color\n",
    "\n",
    "\n",
    "for id_class in range(1,seg_map.max()+1):\n",
    "    rgb_image[seg_map == id_class] = [np.random.randint(255), np.random.randint(255), np.random.randint(255)]\n",
    "\n",
    "# save and display\n",
    "\n",
    "cv2.imwrite(\"annotation.png\",rgb_image)\n",
    "cv2.imwrite(\"mix.png\",(rgb_image/2+image/2).astype(np.uint8))\n",
    "\n",
    "cv2.imshow(\"annotation\",rgb_image)\n",
    "cv2.imshow(\"mix\",(rgb_image/2+image/2).astype(np.uint8))\n",
    "cv2.imshow(\"image\",image)\n",
    "cv2.waitKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79404bc3-ec15-46ea-bf3e-c5f6a82cbf04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
